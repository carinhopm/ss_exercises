{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python385jvsc74a57bd09bacb6458dda4b42c6a07503ac3588add1d91b29c1d69c745adbc07a0974aff1",
   "display_name": "Python 3.8.5 64-bit ('base': conda)"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import scipy.stats as st"
   ]
  },
  {
   "source": [
    "1. Estimate the integral by simulation (the crude Monte Carlo estimator). Use eg. an estimator based on 100 samples and present the result as the point estimator and a confidence interval.\n"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "The crude Monte Carlo estimator: 1.758385394625298\nVariance: 0.24163717464519563\nConfidence Interval: (1.6603566518532, 1.8564141373973975)\n"
     ]
    }
   ],
   "source": [
    "u = np.random.uniform(size=100)\n",
    "y = np.exp(u)\n",
    "print(f'The crude Monte Carlo estimator: {sum(y)/len(y)}')\n",
    "print(f'Variance: {np.var(y)}')\n",
    "print(f'Confidence Interval: {st.t.interval(0.95, len(y)-1, loc=np.mean(y), scale=st.sem(y))}')"
   ]
  },
  {
   "source": [
    "2. Estimate the integral using antithetic variables, with comparable computer ressources."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Estimator with Antithetic Variables: 1.724974966472243\nVariance: 0.003973735199904254\nConfidence Interval: (1.712403931923072, 1.7375460010214117)\n"
     ]
    }
   ],
   "source": [
    "u = np.random.uniform(size=100)\n",
    "y = (np.exp(u) + np.exp(1-u)) / 2\n",
    "print(f'Estimator with Antithetic Variables: {sum(y)/len(y)}')\n",
    "print(f'Variance: {np.var(y)}')\n",
    "print(f'Confidence Interval: {st.t.interval(0.95, len(y)-1, loc=np.mean(y), scale=st.sem(y))}')"
   ]
  },
  {
   "source": [
    "3. Estimate the integral using a control variable, with comparable computer ressources.\n"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Estimator with Control Variable: 1.7051512598306309\nVariance: 0.003691562279328948\nConfidence Interval: (1.6930347745764014, 1.7172677450848604)\n"
     ]
    }
   ],
   "source": [
    "u = np.random.uniform(size=100)\n",
    "x = np.exp(u)\n",
    "y = u\n",
    "cov = np.mean(x*y) - (np.mean(x)*np.mean(y))\n",
    "c = -cov / np.var(y)\n",
    "mu_y = np.mean(y)\n",
    "z = x + c*(y-mu_y)\n",
    "print(f'Estimator with Control Variable: {sum(z)/len(z)}')\n",
    "print(f'Variance: {np.var(z)}')\n",
    "print(f'Confidence Interval: {st.t.interval(0.95, len(z)-1, loc=np.mean(z), scale=st.sem(z))}')"
   ]
  },
  {
   "source": [
    "4. Estimate the integral using stratified sampling, with comparable computer ressources.\n"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Estimator with Stratified Sampling: 1.7226715363368985\nVariance: 0.002276484561244034\nConfidence Interval: (1.7131566432458274, 1.7321864294279705)\n"
     ]
    }
   ],
   "source": [
    "def rd():\n",
    "    return np.random.uniform()\n",
    "\n",
    "y = []\n",
    "for i in range(100):\n",
    "    num = np.exp(rd()/5) + np.exp(1/5+rd()/5) + np.exp(2/5+rd()/5) + np.exp(3/5+rd()/5) + np.exp(4/5+rd()/5)\n",
    "    y.append(num / 5)\n",
    "y = np.asarray(y)\n",
    "print(f'Estimator with Stratified Sampling: {sum(y)/len(y)}')\n",
    "print(f'Variance: {np.var(y)}')\n",
    "print(f'Confidence Interval: {st.t.interval(0.95, len(y)-1, loc=np.mean(y), scale=st.sem(y))}')"
   ]
  },
  {
   "source": [
    "5. Use control variates to reduce the variance of the estimator in exercise 4 (Poisson arrivals)."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "source": [
    "6. Demonstrate the effect of using common random numbers in exercise 4 for the difference between Poisson arrivals (Part 1) and a renewal process with hyperexponential interarrival times. Remark: You might need to some thinking and some re-programming."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "source": [
    "7. Use importance sampling with g(x) = λ exp (−λ ∗ x) to calculate the integral. Try to find the optimal value of λ by calculating the variance of h(x)f(x)/g(x) and verify by simulation. Note that importance sampling with the exponential distribution will not reduce the variance in this case compared to the other methods."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Estimator with Importance Sampling: 1.0335275917514253\nVariance: 0.013279038277452693\nConfidence Interval: (1.010547335368431, 1.0565078481344194)\n"
     ]
    }
   ],
   "source": [
    "u = np.random.uniform(size=100)\n",
    "x = np.exp(u)\n",
    "lamda = 1 # at least for the combination of f and g this is not affecting\n",
    "f_x = lamda*np.exp(-lamda*x)\n",
    "g_x = lamda*np.exp(-lamda*x) + 0.1 # if we increase this difference the variance decreases\n",
    "z = (x*f_x) / g_x\n",
    "print(f'Estimator with Importance Sampling: {sum(z)/len(z)}')\n",
    "print(f'Variance: {np.var(z)}')\n",
    "print(f'Confidence Interval: {st.t.interval(0.95, len(z)-1, loc=np.mean(z), scale=st.sem(z))}')"
   ]
  },
  {
   "source": [
    "8. Estimate the probability X > a. For a standard normal random variable Z ∼ N(0, 1) using the crude Monte Carlo estimator. Then try importance sampling with a normal density with mean a and\n",
    "variance σ2. For the expirements start using σ2 = 1, use different values of a (e.g. 2 and 4), and different sample sizes. If time permits experiment with other values for σ2. Finally discuss the efficiency of the methods."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "'f_x = lamda*np.exp(−lamda*x)'"
      ]
     },
     "metadata": {},
     "execution_count": 29
    }
   ],
   "source": []
  },
  {
   "source": [
    "9. For the Pareto case derive the IS estimator for the mean using the first moment distribution as sampling distribution. Is the approach meaningful? and could this be done in general?"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ]
}